{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T15:23:38.277042Z",
     "start_time": "2023-07-16T15:23:36.692740Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from scipy.spatial.distance import squareform,pdist\n",
    "from numpy.linalg import norm\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import models\n",
    "from models import InvertibleNN\n",
    "from EI_calculation import approx_ei\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0') if use_cuda else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T15:23:38.331272Z",
     "start_time": "2023-07-16T15:23:38.280146Z"
    }
   },
   "outputs": [],
   "source": [
    "class Parellel_Renorm_Dynamic(nn.Module):\n",
    "    def __init__(self, sym_size, latent_size, effect_size, cut_size, hidden_units,normalized_state,device,is_random=False):\n",
    "        #latent_size: input size\n",
    "        #effect_size: scale, effective latent dynamics size\n",
    "        super(Parellel_Renorm_Dynamic, self).__init__()\n",
    "        if latent_size < 1 or latent_size > sym_size:\n",
    "            print('Latent Size is too small(<1) or too large(>input_size):', latent_size)\n",
    "            raise\n",
    "            return\n",
    "        \n",
    "        self.device = device\n",
    "        self.latent_size = latent_size\n",
    "        self.effect_size = effect_size\n",
    "        self.sym_size = sym_size\n",
    "        i = sym_size\n",
    "        flows = []\n",
    "        dynamics_modules = []\n",
    "        inverse_dynamics_modules = []\n",
    "        \n",
    "        while i > latent_size:    \n",
    "            input_size = max(latent_size, i)\n",
    "            if i == sym_size:\n",
    "                mid_size = sym_size\n",
    "                dynamics = self.build_dynamics(mid_size, hidden_units)\n",
    "                dynamics_modules.append(dynamics)\n",
    "                inverse_dynamics = self.build_dynamics(mid_size, hidden_units)\n",
    "                inverse_dynamics_modules.append(inverse_dynamics)\n",
    "                flow = self.build_flow(input_size, hidden_units)\n",
    "                flows.append(flow)\n",
    "            \n",
    "            flow = self.build_flow(input_size, hidden_units)\n",
    "            flows.append(flow)\n",
    "            mid_size = max(latent_size, i // cut_size)\n",
    "            dynamics = self.build_dynamics(mid_size, hidden_units)\n",
    "            dynamics_modules.append(dynamics)\n",
    "            inverse_dynamics = self.build_dynamics(mid_size, hidden_units)\n",
    "            inverse_dynamics_modules.append(inverse_dynamics)\n",
    "            i = i // cut_size\n",
    "        self.flows = nn.ModuleList(flows)\n",
    "        self.dynamics_modules = nn.ModuleList(dynamics_modules)\n",
    "        self.inverse_dynamics_modules = nn.ModuleList(inverse_dynamics_modules)\n",
    "        \n",
    "        self.normalized_state=normalized_state\n",
    "        self.is_random = is_random\n",
    "    def build_flow(self, input_size, hidden_units):\n",
    "        if input_size % 2 !=0 and input_size > 1:\n",
    "            input_size = input_size - 1\n",
    "        nets = lambda: nn.Sequential(nn.Linear(input_size, hidden_units), nn.LeakyReLU(), \n",
    "                                     nn.Linear(hidden_units, hidden_units), nn.LeakyReLU(), \n",
    "                                     nn.Linear(hidden_units, input_size), nn.Tanh())\n",
    "        nett = lambda: nn.Sequential(nn.Linear(input_size, hidden_units), nn.LeakyReLU(), \n",
    "                                     nn.Linear(hidden_units, hidden_units), nn.LeakyReLU(), \n",
    "                                     nn.Linear(hidden_units, input_size))\n",
    "\n",
    "        mask1 = torch.cat((torch.zeros(1, input_size // 2, device=self.device), \n",
    "                           torch.ones(1, input_size // 2, device=self.device)), 1)\n",
    "        mask2 = 1 - mask1\n",
    "        masks = torch.cat((mask1, mask2, mask1, mask2, mask1, mask2), 0)\n",
    "        flow = InvertibleNN(nets, nett, masks, self.device)\n",
    "        return flow\n",
    "    def build_dynamics(self, mid_size, hidden_units):\n",
    "        dynamics = nn.Sequential(nn.Linear(mid_size, hidden_units), nn.LeakyReLU(), \n",
    "                                     nn.Linear(hidden_units, hidden_units), nn.LeakyReLU(), \n",
    "                                     nn.Linear(hidden_units, mid_size))\n",
    "        return dynamics\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #state_dim = x.size()[1]\n",
    "        \n",
    "        if len(x.size())<=1:\n",
    "            x = x.unsqueeze(0)\n",
    "        ss = self.encoding(x)\n",
    "        \n",
    "        s_nexts = []\n",
    "        ys = []\n",
    "        for i,s in enumerate(ss):\n",
    "            s_next = self.dynamics_modules[i](s) + s\n",
    "            if self.normalized_state:\n",
    "                s_next = torch.tanh(s_next)\n",
    "            if self.is_random:\n",
    "                s_next = s_next + torch.relu(self.sigmas.repeat(s_next.size()[0],1)) * torch.randn(s_next.size(),\n",
    "                                                                                                   device=self.device)\n",
    "            if i > 0:\n",
    "                y = self.decoding(s_next, i)\n",
    "            else:\n",
    "                y = s_next\n",
    "            s_nexts.append(s_next)\n",
    "            ys.append(y)\n",
    "        return ys, ss, s_nexts\n",
    "    def back_forward(self, x):\n",
    "        #state_dim = x.size()[1]\n",
    "        \n",
    "        if len(x.size())<=1:\n",
    "            x = x.unsqueeze(0)\n",
    "        ss = self.encoding(x)\n",
    "        \n",
    "        s_nexts = []\n",
    "        ys = []\n",
    "        for i,s in enumerate(ss):\n",
    "            s_next = self.inverse_dynamics_modules[i](s) - s\n",
    "            if self.normalized_state:\n",
    "                s_next = torch.tanh(s_next)\n",
    "            if self.is_random:\n",
    "                s_next = s_next + torch.relu(self.sigmas.repeat(s_next.size()[0],1)) * torch.randn(s_next.size(),\n",
    "                                                                                                   device=self.device)\n",
    "            if i > 0:\n",
    "                y = self.decoding(s_next, i)\n",
    "            else:\n",
    "                y = s_next\n",
    "            s_nexts.append(s_next)\n",
    "            ys.append(y)\n",
    "        return ys, ss, s_nexts\n",
    "    def decoding(self, s_next, level):\n",
    "        y = s_next\n",
    "        for i in range(level+1)[::-1]:\n",
    "            flow = self.flows[i]\n",
    "            end_size = self.latent_size\n",
    "            if i < len(self.flows)-1:\n",
    "                flow_n = self.flows[i+1]\n",
    "                end_size = max(y.size()[1], flow_n.size)\n",
    "            #print(flow.size, end_size, y.size()[1])\n",
    "            sz = flow.size - end_size\n",
    "            \n",
    "            if sz>0:\n",
    "                noise = distributions.MultivariateNormal(torch.zeros(sz), torch.eye(sz)).sample((y.size()[0], 1))\n",
    "                noise = noise.to(self.device)\n",
    "                #print(noise.size(), s_next.size(1))\n",
    "                if y.size()[0]>1:\n",
    "                    noise = noise.squeeze(1)\n",
    "                else:\n",
    "                    noise = noise.squeeze(0)\n",
    "                y = torch.cat((y, noise), 1)\n",
    "            y,_ = flow.g(y)\n",
    "        return y\n",
    "    def decoding1(self, s_next, level):\n",
    "        y = s_next\n",
    "        for i in range(level+1)[::-1]:\n",
    "            flow = self.flows[i]\n",
    "            end_size = self.latent_size\n",
    "            if i < len(self.flows)-1:\n",
    "                flow_n = self.flows[i+1]\n",
    "                end_size = max(y.size()[1], flow_n.size)\n",
    "            #print(flow.size, end_size, y.size()[1])\n",
    "            sz = flow.size - end_size\n",
    "            \n",
    "            if sz>0:\n",
    "                noise = distributions.MultivariateNormal(torch.zeros(sz), torch.eye(sz)).sample((y.size()[0], 1))\n",
    "                noise = noise.to(self.device)\n",
    "                #print(noise.size(), s_next.size(1))\n",
    "                if y.size()[0]>1:\n",
    "                    noise = noise.squeeze(1)\n",
    "                else:\n",
    "                    noise = noise.squeeze(0)\n",
    "                y = torch.cat((y, noise), 1)\n",
    "            y,_ = flow.g(y)\n",
    "        return y\n",
    "    def encoding(self, x):\n",
    "        xx = x\n",
    "        if len(x.size()) > 1:\n",
    "            if x.size()[1] < self.sym_size:\n",
    "                xx = torch.cat((x, torch.zeros([x.size()[0], self.sym_size - x.size()[1]], device=self.device)), 1)\n",
    "        else:\n",
    "            if x.size()[0] < self.sym_size:\n",
    "                xx = torch.cat((x, torch.zeros([self.sym_size - x.size()[0]], device=self.device)), 0)\n",
    "        y = xx\n",
    "        ys = []\n",
    "        for i,flow in enumerate(self.flows):\n",
    "            if y.size()[1] > flow.size:\n",
    "                #y = torch.cat((y, y[:,:1]), 1)\n",
    "                y = y[:, :flow.size]\n",
    "            y,_ = flow.f(y)\n",
    "            if self.normalized_state:\n",
    "                y = torch.tanh(y)\n",
    "            pdict = dict(self.dynamics_modules[i].named_parameters())\n",
    "            lsize = pdict['0.weight'].size()[1]\n",
    "            y = y[:, :lsize]\n",
    "            ys.append(y)\n",
    "        return ys\n",
    "    def encoding1(self, x):\n",
    "        xx = x\n",
    "        if len(x.size()) > 1:\n",
    "            if x.size()[1] < self.sym_size:\n",
    "                xx = torch.cat((x, torch.zeros([x.size()[0], self.sym_size - x.size()[1]], device=self.device)), 1)\n",
    "        else:\n",
    "            if x.size()[0] < self.sym_size:\n",
    "                xx = torch.cat((x, torch.zeros([self.sym_size - x.size()[0]], device=self.device)), 0)\n",
    "        y = xx\n",
    "        ys = []\n",
    "        for i,flow in enumerate(self.flows):\n",
    "            if y.size()[1] > flow.size:\n",
    "                #y = torch.cat((y, y[:,:1]), 1)\n",
    "                y = y[:, :flow.size]\n",
    "            y,_ = flow.f(y)\n",
    "            if self.normalized_state:\n",
    "                y = torch.tanh(y)\n",
    "            pdict = dict(self.inverse_dynamics_modules[i].named_parameters())\n",
    "            lsize = pdict['0.weight'].size()[1]\n",
    "            y = y[:, :lsize]\n",
    "            ys.append(y)\n",
    "        return ys\n",
    "    def loss(self, predictions, real, loss_f):\n",
    "        losses = []\n",
    "        sum_loss = 0\n",
    "        for i, predict in enumerate(predictions):\n",
    "            loss = loss_f(real, predict)\n",
    "            losses.append(loss)\n",
    "            sum_loss += loss\n",
    "        return losses, sum_loss / len(predictions)\n",
    "    def loss_weights(self, predictions, real, weightss,loss_f,level):\n",
    "        losses = []\n",
    "        sum_loss = 0\n",
    "        for i, predict in enumerate(predictions):\n",
    "            if i==level:\n",
    "                w=weightss[0]\n",
    "                loss = (loss_f(real, predict)*w).mean([0,1])\n",
    "            else:\n",
    "                loss = loss_f(real, predict).mean([0,1])\n",
    "            losses.append(loss)\n",
    "            sum_loss += loss\n",
    "        return losses, sum_loss / len(predictions)\n",
    "    def loss_weights_back(self, predictions, real,weightss, loss_f,level):\n",
    "        losses = []\n",
    "        sum_loss = 0\n",
    "        for i, predict in enumerate(predictions):\n",
    "            if i==level:\n",
    "                w=weightss[0]\n",
    "                loss = (loss_f(real, predict)*w).mean([0,1])\n",
    "            else:\n",
    "                loss = loss_f(real, predict).mean([0,1])\n",
    "            losses.append(loss)\n",
    "            sum_loss += loss\n",
    "        return losses, sum_loss / len(predictions)\n",
    "    def calc_EIs(self, real, latent_ps, device):\n",
    "        sp = self.encoding(real)\n",
    "        eis = []\n",
    "        sigmass = []\n",
    "        scales = []\n",
    "        for i,state in enumerate(sp):\n",
    "            latent_p = latent_ps[i]\n",
    "            flow = self.flows[i]\n",
    "            dynamics = self.dynamics_modules[i]\n",
    "            dd = dict(dynamics.named_parameters())\n",
    "            scale = dd['0.weight'].size()[1]\n",
    "            \n",
    "            sigmas = torch.sqrt(torch.mean((state-latent_p)**2, 0))\n",
    "            sigmas_matrix = torch.diag(sigmas)\n",
    "            ei = approx_ei(scale, scale, sigmas_matrix.data, lambda x:(dynamics(x.unsqueeze(0))+x.unsqueeze(0)), \n",
    "                           num_samples = 1000, L=100, easy=True, device=device)\n",
    "            eis.append(ei)\n",
    "            sigmass.append(sigmas)\n",
    "            scales.append(scale)\n",
    "        return eis, sigmass, scales\n",
    "    \n",
    "    def to_weights(self,log_w, temperature=10):  #？重加权应该是个优化模型\n",
    "        #将log_w做softmax归一化，得到权重\n",
    "        logsoft = nn.LogSoftmax(dim = 0)\n",
    "        weights = torch.exp(logsoft(log_w/temperature))\n",
    "        return weights\n",
    "    def kde_density(self,X):\n",
    "        is_cuda = X.is_cuda  #True为储存在GPU\n",
    "        ldev = X.device  #分配内存在哪里运行\n",
    "        dim = X.size()[1] #获取数据的列数即维数\n",
    "        # kde = KernelDensity(kernel='gaussian', bandwidth=0.1, atol=0.005).fit(X.cpu().data.numpy())\n",
    "        kde = KernelDensity(kernel='gaussian', bandwidth=0.05, atol=0.2).fit(X.cpu().data.numpy())\n",
    "        log_density = kde.score_samples(X.cpu().data.numpy())\n",
    "        return log_density, kde\n",
    "    def calc_EIs_ked(self,s,sp,lp,samples,MAE_raw,L,bigL,level,device):\n",
    "        #spring_data生成方式：spring_data = spring.generate_multistep(size=1000, steps=10, sigma=sigma, lam=1,miu=0.5)，为多步数据，样本点等于size*steps\n",
    "        #samples跟spring_data样本点数一样，数越大运算会越慢，主要是kde估计很耗时间\n",
    "        #MAE_raw = torch.nn.L1Loss(reduction='none')\n",
    "        #L=bigL,根据隐空间范围进行调试，要保证能覆盖隐空间同时尽可能小\n",
    "        \n",
    "        encodes=self.encoding(sp)\n",
    "        predicts1, latent1s, latentp1s = self.forward(s)\n",
    "        eis=[]\n",
    "        sigmass = []\n",
    "        weightss=[]\n",
    "        for index in range(len(predicts1)):\n",
    "            if index==level:\n",
    "                sigmas_matrix=torch.zeros([2,2],device=device)\n",
    "                dynamics = self.dynamics_modules[index]\n",
    "                latent1=latent1s[index]\n",
    "                latentp1=latentp1s[index]\n",
    "                scale=len(latent1[0])\n",
    "                encode=encodes[index]\n",
    "\n",
    "                log_density, k_model_n = self.kde_density(latent1)\n",
    "                log_rho = - scale * torch.log(2.0*torch.from_numpy(np.array(L)))  #均匀分布的概率分布\n",
    "                logp = log_rho - log_density  #两种概率分布的差\n",
    "                weights = self.to_weights(logp, temperature=1) * samples\n",
    "                if use_cuda:\n",
    "                    weights = weights.cuda(device=device)\n",
    "                weights=weights.unsqueeze(1)\n",
    "                \n",
    "                mae1 = MAE(latentp1, encode) * torch.cat([weights for i in range(scale)],1) \n",
    "                #mae1 = MAE_raw(torch.cat([latentp1 for i in range(len(latent1s[0][0]))],0), encode) * torch.cat(\n",
    "                #    [torch.cat([weights for i in range(scale)],1) for j in range(len(latent1s[0][0]))],0)  \n",
    "                    #两维的情况，根据维度情况需要调整，weights的多维直接copy就行\n",
    "                sigmas=mae1.mean(axis=0)\n",
    "                \n",
    "                sigmas_matrix = torch.diag(sigmas)\n",
    "                ei = approx_ei(scale, scale, sigmas_matrix.data, lambda x:(dynamics(x.unsqueeze(0))+x.unsqueeze(0)), \n",
    "                            num_samples = 1000, L=bigL, easy=True, device=device)  #approx_ei函数本身没有变化\n",
    "        \n",
    "                eis.append(ei)\n",
    "                sigmass.append(sigmas)\n",
    "                weightss.append(weights)\n",
    "            else:\n",
    "                latent_p = lp[index]\n",
    "                flow = self.flows[index]\n",
    "                dynamics = self.dynamics_modules[index]\n",
    "                dd = dict(dynamics.named_parameters())\n",
    "                scale = dd['0.weight'].size()[1]\n",
    "                \n",
    "                sigmas = torch.sqrt(torch.mean((encodes[index]-latent_p)**2, 0))\n",
    "                sigmas_matrix = torch.diag(sigmas)\n",
    "                ei = approx_ei(scale, scale, sigmas_matrix.data, lambda x:(dynamics(x.unsqueeze(0))+x.unsqueeze(0)), \n",
    "                            num_samples = 1000, L=100, easy=True, device=device)\n",
    "                eis.append(ei)\n",
    "                sigmass.append(sigmas)\n",
    "                #scales.append(scale)\n",
    "        return eis, sigmass,weightss\n",
    "     \n",
    "    def eff_predict(self, prediction):\n",
    "        return prediction[:, :self.effect_size]\n",
    "    def simulate(self, x, level):\n",
    "        if level > len(self.dynamics_modules) or level<0:\n",
    "            print('input error: level must be less than', len(self.dynamics_modules))\n",
    "        dynamics = self.dynamics_modules[level]\n",
    "        x_next = dynamics(x) + x\n",
    "        decode = self.decoding(x_next, level)\n",
    "        return x_next, decode\n",
    "    def multi_step_prediction(self, s, steps, level):\n",
    "        if level > len(self.dynamics_modules) or level<0:\n",
    "            print('input error: level must be less than', len(self.dynamics_modules))\n",
    "        s_hist = s\n",
    "        ss = self.encoding(s)\n",
    "        z_hist = ss[level]\n",
    "        z = z_hist[:1, :]\n",
    "        for t in range(steps):    \n",
    "            z_next, s_next = self.simulate(z, level)\n",
    "            z_hist = torch.cat((z_hist, z_next), 0)\n",
    "            s_hist = torch.cat((s_hist, self.eff_predict(s_next)), 0)\n",
    "            z = z_next\n",
    "        return s_hist, z_hist\n",
    "    def multi_step_prediction_0(self, s, steps, level):\n",
    "        if level > len(self.dynamics_modules) or level<0:\n",
    "            print('input error: level must be less than', len(self.dynamics_modules))\n",
    "        predicts=[]\n",
    "        latents=[]\n",
    "        s_hist = s\n",
    "        ss = self.encoding(s)\n",
    "        z_hist = ss[level]\n",
    "        z = z_hist[:, :]\n",
    "        predicts.append(s)\n",
    "        latents.append(z)\n",
    "        for t in range(steps):    \n",
    "            z_next, s_next = self.simulate(z, level)\n",
    "            z_hist = torch.cat((z_hist, z_next), 0)\n",
    "            s_hist = torch.cat((s_hist, self.eff_predict(s_next)), 0)\n",
    "            predicts.append(s_next)\n",
    "            latents.append(z_next)\n",
    "            z = z_next\n",
    "        return predicts, latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T15:23:38.351555Z",
     "start_time": "2023-07-16T15:23:38.333812Z"
    }
   },
   "outputs": [],
   "source": [
    "width, height, init_w = 300,300, 5\n",
    "\n",
    "class Boids1:\n",
    "    '''initialize the boids simulation'''\n",
    "    def __init__(self,N):\n",
    "        #initialize the boid simulation, in 4 groups\n",
    "        self.pos =init_w*np.random.rand(2*N).reshape(N,2)\n",
    "        #init_center = np.array([[width//4, height//4],\n",
    "        #                        [width//4, height*3//4],\n",
    "        #                       [width*3//4, height//4],\n",
    "        #                       [width*3//4, height*3//4]])\n",
    "        init_center = np.array([[width//2-width//150, height//2]])\n",
    "        #init_center=height*np.random.rand(2*4).reshape(4,2)\n",
    "        for i in range(1):\n",
    "            self.pos[N*i//1:N*(i+1)//1] += init_center[i] \n",
    "        #self.angles = 2*math.pi*np.array([np.random.rand() for i in range(N)])\n",
    "        self.angles = 2*math.pi*np.array([1/2 for i in range(N)])\n",
    "        angles= self.angles\n",
    "        #angles = 2*math.pi*np.array([1/8 for i in range(N)])\n",
    "        self.vel = np.array(list(zip(np.sin(angles),np.cos(angles))))\n",
    "        self.N=N\n",
    "#控制速度恒定为2\n",
    "        self.velFix = 1.0 \n",
    "#排斥和内聚（以及平行）的检测距离        \n",
    "        self.minDist =1\n",
    "        self.maxDist = 80\n",
    "        self.sepDist = 180\n",
    "#平行，排斥，内聚, 群排斥速度的权重\n",
    "        self.al = 1.0\n",
    "        self.se = 1.0\n",
    "        self.co = 1.0\n",
    "        self.se2 = 0.5\n",
    "        self.bumper = 0.3\n",
    "        self.maxRuleVel = 0.5\n",
    "        self.maxVel = 2.0\n",
    "#噪声\n",
    "        self.noise = 0.2\n",
    "#平均速度和平均距离（初始设置为0）\n",
    "        self.aveVel = 0\n",
    "        self.aveDist = 0\n",
    "        \n",
    "    def move(self):\n",
    "#update the simulation by one time step\n",
    "#距离矩阵，使用 squareform(pdist(self.pos))能直接输出类鸟间相对距离的矩阵，输出的矩阵对角线为0且对称\n",
    "        self.distMatrix = squareform(pdist(self.pos))\n",
    "        self.aveDist = self.distMatrix.sum()/(self.N)**2\n",
    "        #apply rules\n",
    "        self.vel +=self.applyRules()\n",
    "        self.fix(self.vel,self.velFix)\n",
    "#加上噪声\n",
    "        #angles = 2*math.pi*np.random.rand(self.N)\n",
    "        angles=self.angles\n",
    "        self.vel += np.array(list(zip(np.sin(angles),np.cos(angles))))*self.noise*self.velFix\n",
    "        self.angles = self.angles+2*math.pi*np.array([1/256 for i in range(self.N)])\n",
    "#控制速度一定\n",
    "        self.fix(self.vel,self.velFix)\n",
    "        self.aveVel = norm(self.vel.sum(axis=0)/self.N)\n",
    "        self.pos +=self.vel\n",
    "#周期边界条件\n",
    "        self.applyBC()     \n",
    "        \n",
    "    def limitVec(self,vec,maxVal):\n",
    "        #limit the magnitide of the 2D vector\n",
    "        mag = norm(vec)\n",
    "        #将速度换成maxVal\n",
    "        if mag > maxVal:\n",
    "            vec[0],vec[1] = vec[0]*maxVal/mag,vec[1]*maxVal/mag\n",
    "            \n",
    "    def limit(self,X,maxVal):\n",
    "        #limit the magnitide of 2D vectors in array X to maxVal\n",
    "        for vec in X:\n",
    "            self.limitVec(vec,maxVal)\n",
    "            \n",
    "    def fixVec(self,vec,velFix):\n",
    "        #limit the magnitide of the 2D vector\n",
    "        mag = norm(vec)\n",
    "#将速度换成velFix\n",
    "        vec[0],vec[1] = vec[0]*velFix/mag,vec[1]*velFix/mag\n",
    "            \n",
    "    def fix(self,X,velFix):\n",
    "        #limit the magnitide of 2D vectors in array X to maxVal\n",
    "        for vec in X:\n",
    "            self.fixVec(vec,velFix)\n",
    "            \n",
    "    def applyBC(self):\n",
    "        #apply boundary conditions\n",
    "        deltaR = 5\n",
    "        for i in range(len(self.pos)):\n",
    "            coord = self.pos[i]\n",
    "            vel = self.vel[i]\n",
    "            if coord[0] > width - deltaR:\n",
    "                # coord[0] = -deltaR\n",
    "                vel[0] -= self.bumper * self.velFix * (coord[0] + deltaR - width) / deltaR\n",
    "                \n",
    "            if coord[0] < deltaR:\n",
    "                # coord[0] = width+deltaR\n",
    "                vel[0] += self.bumper * self.velFix * (deltaR - coord[0]) / deltaR\n",
    "                \n",
    "            if coord[1] > height - deltaR:\n",
    "                # coord[1] = -deltaR\n",
    "                vel[1] -= self.bumper * self.velFix * (coord[1] + deltaR - height) / deltaR\n",
    "                \n",
    "            if coord[1] < deltaR:\n",
    "                # coord[1] = height+deltaR\n",
    "                vel[1] += self.bumper * self.velFix * (deltaR - coord[1]) / deltaR\n",
    "            \n",
    "    def applyRules(self):\n",
    "        #rule 1:separation\n",
    "        D = self.distMatrix <self.minDist\n",
    "#求解质心位置，并且产生远离质心的速度\n",
    "        vel = self.pos*D.sum(axis=1).reshape(self.N,1) - D.dot(self.pos)\n",
    "        self.limit(vel,self.maxRuleVel)\n",
    "        vel = vel*self.se\n",
    "        D = self.distMatrix <self.maxDist\n",
    "        #rule 2:alignment\n",
    "#求范围内平均速度\n",
    "        vel2 = D.dot(self.vel)\n",
    "        self.limit(vel2,self.maxRuleVel)\n",
    "        vel +=vel2*self.al\n",
    "        #rule 3: cohesion\n",
    "#朝着质心移动\n",
    "        # Yuan Bing: keep the same scale as of vel1\n",
    "        vel3 =D.dot(self.pos) - self.pos*D.sum(axis=1).reshape(self.N,1)#质心-自己位置\n",
    "        self.limit(vel3,self.maxRuleVel)\n",
    "        vel +=vel3*self.co\n",
    "#远离质心移动\n",
    "        D = self.distMatrix < self.sepDist\n",
    "        vel4 = self.pos*D.sum(axis=1).reshape(self.N,1) - D.dot(self.pos)#自己位置-质心\n",
    "        self.limit(vel4,self.maxRuleVel)\n",
    "        vel +=vel4*self.se2\n",
    "        self.limit(vel,self.maxRuleVel)\n",
    "        return vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T15:23:38.371964Z",
     "start_time": "2023-07-16T15:23:38.354518Z"
    }
   },
   "outputs": [],
   "source": [
    "width, height, init_w = 300,300, 5\n",
    "\n",
    "class Boids2:\n",
    "    '''initialize the boids simulation'''\n",
    "    def __init__(self,N):\n",
    "        #initialize the boid simulation, in 4 groups\n",
    "        self.pos =init_w*np.random.rand(2*N).reshape(N,2)\n",
    "        #init_center = np.array([[width//4, height//4],\n",
    "        #                        [width//4, height*3//4],\n",
    "        #                       [width*3//4, height//4],\n",
    "        #                       [width*3//4, height*3//4]])\n",
    "        init_center = np.array([[width//2+width//150, height//2]])\n",
    "        #init_center=height*np.random.rand(2*4).reshape(4,2)\n",
    "        for i in range(1):\n",
    "            self.pos[N*i//1:N*(i+1)//1] += init_center[i] \n",
    "        #self.angles = 2*math.pi*np.array([np.random.rand() for i in range(N)])\n",
    "        self.angles = -2*math.pi*np.array([1/2 for i in range(N)])\n",
    "        angles= self.angles\n",
    "        #angles = 2*math.pi*np.array([1/8 for i in range(N)])\n",
    "        self.vel = np.array(list(zip(np.sin(angles),np.cos(angles))))\n",
    "        self.N=N\n",
    "#控制速度恒定为2\n",
    "        self.velFix = 1.0 \n",
    "#排斥和内聚（以及平行）的检测距离        \n",
    "        self.minDist =1\n",
    "        self.maxDist = 80\n",
    "        self.sepDist = 180\n",
    "#平行，排斥，内聚, 群排斥速度的权重\n",
    "        self.al = 1.0\n",
    "        self.se = 1.0\n",
    "        self.co = 1.0\n",
    "        self.se2 = 0.5\n",
    "        self.bumper = 0.3\n",
    "        self.maxRuleVel = 0.5\n",
    "        self.maxVel = 2.0\n",
    "#噪声\n",
    "        self.noise = 0.2\n",
    "#平均速度和平均距离（初始设置为0）\n",
    "        self.aveVel = 0\n",
    "        self.aveDist = 0\n",
    "        \n",
    "    def move(self):\n",
    "#update the simulation by one time step\n",
    "#距离矩阵，使用 squareform(pdist(self.pos))能直接输出类鸟间相对距离的矩阵，输出的矩阵对角线为0且对称\n",
    "        self.distMatrix = squareform(pdist(self.pos))\n",
    "        self.aveDist = self.distMatrix.sum()/(self.N)**2\n",
    "        #apply rules\n",
    "        self.vel +=self.applyRules()\n",
    "        self.fix(self.vel,self.velFix)\n",
    "#加上噪声\n",
    "        #angles = 2*math.pi*np.random.rand(self.N)\n",
    "        angles=self.angles\n",
    "        self.vel += np.array(list(zip(np.sin(angles),np.cos(angles))))*self.noise*self.velFix\n",
    "        self.angles = self.angles-2*math.pi*np.array([1/192 for i in range(self.N)])\n",
    "#控制速度一定\n",
    "        self.fix(self.vel,self.velFix)\n",
    "        self.aveVel = norm(self.vel.sum(axis=0)/self.N)\n",
    "        self.pos +=self.vel\n",
    "#周期边界条件\n",
    "        self.applyBC()     \n",
    "        \n",
    "    def limitVec(self,vec,maxVal):\n",
    "        #limit the magnitide of the 2D vector\n",
    "        mag = norm(vec)\n",
    "        #将速度换成maxVal\n",
    "        if mag > maxVal:\n",
    "            vec[0],vec[1] = vec[0]*maxVal/mag,vec[1]*maxVal/mag\n",
    "            \n",
    "    def limit(self,X,maxVal):\n",
    "        #limit the magnitide of 2D vectors in array X to maxVal\n",
    "        for vec in X:\n",
    "            self.limitVec(vec,maxVal)\n",
    "            \n",
    "    def fixVec(self,vec,velFix):\n",
    "        #limit the magnitide of the 2D vector\n",
    "        mag = norm(vec)\n",
    "#将速度换成velFix\n",
    "        vec[0],vec[1] = vec[0]*velFix/mag,vec[1]*velFix/mag\n",
    "            \n",
    "    def fix(self,X,velFix):\n",
    "        #limit the magnitide of 2D vectors in array X to maxVal\n",
    "        for vec in X:\n",
    "            self.fixVec(vec,velFix)\n",
    "            \n",
    "    def applyBC(self):\n",
    "        #apply boundary conditions\n",
    "        deltaR = 5\n",
    "        for i in range(len(self.pos)):\n",
    "            coord = self.pos[i]\n",
    "            vel = self.vel[i]\n",
    "            if coord[0] > width - deltaR:\n",
    "                # coord[0] = -deltaR\n",
    "                vel[0] -= self.bumper * self.velFix * (coord[0] + deltaR - width) / deltaR\n",
    "                \n",
    "            if coord[0] < deltaR:\n",
    "                # coord[0] = width+deltaR\n",
    "                vel[0] += self.bumper * self.velFix * (deltaR - coord[0]) / deltaR\n",
    "                \n",
    "            if coord[1] > height - deltaR:\n",
    "                # coord[1] = -deltaR\n",
    "                vel[1] -= self.bumper * self.velFix * (coord[1] + deltaR - height) / deltaR\n",
    "                \n",
    "            if coord[1] < deltaR:\n",
    "                # coord[1] = height+deltaR\n",
    "                vel[1] += self.bumper * self.velFix * (deltaR - coord[1]) / deltaR\n",
    "            \n",
    "    def applyRules(self):\n",
    "        #rule 1:separation\n",
    "        D = self.distMatrix <self.minDist\n",
    "#求解质心位置，并且产生远离质心的速度\n",
    "        vel = self.pos*D.sum(axis=1).reshape(self.N,1) - D.dot(self.pos)\n",
    "        self.limit(vel,self.maxRuleVel)\n",
    "        vel = vel*self.se\n",
    "        D = self.distMatrix <self.maxDist\n",
    "        #rule 2:alignment\n",
    "#求范围内平均速度\n",
    "        vel2 = D.dot(self.vel)\n",
    "        self.limit(vel2,self.maxRuleVel)\n",
    "        vel +=vel2*self.al\n",
    "        #rule 3: cohesion\n",
    "#朝着质心移动\n",
    "        # Yuan Bing: keep the same scale as of vel1\n",
    "        vel3 =D.dot(self.pos) - self.pos*D.sum(axis=1).reshape(self.N,1)#质心-自己位置\n",
    "        self.limit(vel3,self.maxRuleVel)\n",
    "        vel +=vel3*self.co\n",
    "#远离质心移动\n",
    "        D = self.distMatrix < self.sepDist\n",
    "        vel4 = self.pos*D.sum(axis=1).reshape(self.N,1) - D.dot(self.pos)#自己位置-质心\n",
    "        self.limit(vel4,self.maxRuleVel)\n",
    "        vel +=vel4*self.se2\n",
    "        self.limit(vel,self.maxRuleVel)\n",
    "        return vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T15:23:38.507468Z",
     "start_time": "2023-07-16T15:23:38.374123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting boids...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('starting boids...')\n",
    "#set the initial number of boids\n",
    "N = 50\n",
    "boids = Boids1(N)    \n",
    "#set up plot\n",
    "fig = plt.figure(1)\n",
    "ax = plt.axes(xlim=(0,width),ylim = (0,height))\n",
    "#设定身体大小为8    \n",
    "pts, = plt.plot([],[],markersize =8,c='k',marker='o',ls='None')\n",
    "#设定头部大小为3\n",
    "beak, = plt.plot([],[],markersize = 3,c = 'r',marker='o',ls='None')\n",
    "aveVels =[]\n",
    "aveDists =[]\n",
    "frameNums = []\n",
    "def update(frameNum,boids):\n",
    "    #update function for animation\n",
    "    #print(frameNum)\n",
    "    if frameNum%50 == 0:\n",
    "        print('当前帧数{}'.format(frameNum))\n",
    "        print('平均速度为{}'.format(boids.aveVel))\n",
    "        aveVels.append(boids.aveVel)\n",
    "        aveDists.append(boids.aveDist)\n",
    "        frameNums.append(frameNum)\n",
    "    boids.move()\n",
    "    pts.set_data(boids.pos.reshape(2*boids.N)[::2],\n",
    "                    boids.pos.reshape(2*boids.N)[1::2])\n",
    "    vec = boids.pos + 4*boids.vel/boids.maxVel \n",
    "    beak.set_data(vec.reshape(2*boids.N)[::2],\n",
    "                    vec.reshape(2*boids.N)[1::2])  \n",
    "    plt.plot(boids.pos[0][0],boids.pos[0][1],c='y',marker=',',markersize=2)\n",
    "    plt.plot(boids.pos[10][0],boids.pos[10][1],c='b',marker=',',markersize=2)\n",
    "    plt.plot(boids.pos[20][0],boids.pos[20][1],c='r',marker=',',markersize=2)\n",
    "    return pts,beak\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T15:23:38.514956Z",
     "start_time": "2023-07-16T15:23:38.509797Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data_p(boids,device):\n",
    "    boids.move()\n",
    "    pts.set_data(boids.pos.reshape(2*boids.N)[::2],\n",
    "                    boids.pos.reshape(2*boids.N)[1::2])\n",
    "    vec = boids.pos + 4*boids.vel/boids.maxVel \n",
    "    beak.set_data(vec.reshape(2*boids.N)[::2],\n",
    "                    vec.reshape(2*boids.N)[1::2])  \n",
    "    #plt.plot(boids.pos[0][0],boids.pos[0][1],c='y',marker=',',markersize=2)\n",
    "    #plt.plot(boids.pos[10][0],boids.pos[10][1],c='b',marker=',',markersize=2)\n",
    "    #plt.plot(boids.pos[20][0],boids.pos[20][1],c='r',marker=',',markersize=2)\n",
    "\n",
    "    Pos_Vec=np.hstack((np.array(boids.pos),np.array(boids.vel)))\n",
    "    Pos_Vec=torch.FloatTensor(Pos_Vec)\n",
    "    Pos_Vec=Pos_Vec.to(device)\n",
    "    return Pos_Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T15:23:38.528612Z",
     "start_time": "2023-07-16T15:23:38.517854Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data_set(Boids1,Boids2,N1,N2,num_of_PosVec,batch_size):\n",
    "    \n",
    "    Pos_Vecs_data=[[] for i in range(num_of_PosVec)]\n",
    "    Pos_Vecps_data=[[] for i in range(num_of_PosVec)]\n",
    "    Pos_Vecs_batch=[[] for batch in range(batch_size)]\n",
    "    Pos_Vecps_batch=[[] for batch in range(batch_size)]\n",
    "\n",
    "    for batch in range(batch_size):\n",
    "        Pos_Vecs=[]\n",
    "        Pos_Vecps=[]\n",
    "        boids1 = Boids1(N1)  \n",
    "        boids2 = Boids2(N2) \n",
    "        Pos_Vec=np.vstack((np.hstack((np.array(boids1.pos),np.array(boids1.vel))),np.hstack((np.array(boids2.pos),np.array(boids2.vel)))))\n",
    "\n",
    "        Pos_Vec=torch.FloatTensor(Pos_Vec)\n",
    "        Pos_Vec=Pos_Vec.to(device)\n",
    "\n",
    "        for origin_t in range(20):\n",
    "            boids1.move()\n",
    "            boids2.move()\n",
    "        for t in range(num_of_PosVec):\n",
    "            Pos_Vec1_p= generate_data_p(boids1,device)\n",
    "            Pos_Vec2_p= generate_data_p(boids2,device)\n",
    "            Pos_Vec_p=torch.cat([Pos_Vec1_p,Pos_Vec2_p], 0)\n",
    "            Pos_Vecs.append(Pos_Vec)\n",
    "            Pos_Vecps.append(Pos_Vec_p)\n",
    "            if t % 50 == 0:\n",
    "                boids1 = Boids1(N1)  \n",
    "                boids2 = Boids2(N2) \n",
    "                for origin_t in range(20):\n",
    "                    boids1.move()\n",
    "                    boids2.move()\n",
    "                Pos_Vec=np.vstack((np.hstack((np.array(boids1.pos),np.array(boids1.vel))),np.hstack((np.array(boids2.pos),np.array(boids2.vel)))))\n",
    "                Pos_Vec=torch.FloatTensor(Pos_Vec)\n",
    "                Pos_Vec=Pos_Vec.to(device)\n",
    "            else:\n",
    "                Pos_Vec=Pos_Vec_p\n",
    "\n",
    "        Pos_Vecs_batch[batch]=Pos_Vecs\n",
    "        Pos_Vecps_batch[batch]=Pos_Vecps\n",
    "\n",
    "    for t in range(num_of_PosVec):\n",
    "        Pos_Vecs_data[t]=torch.stack([Pos_Vecs_batch[batch][t] for batch in range(batch_size)])\n",
    "        Pos_Vecps_data[t]=torch.stack([Pos_Vecps_batch[batch][t] for batch in range(batch_size)])\n",
    "\n",
    "    return Pos_Vecs_data,Pos_Vecps_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T09:35:24.102516Z",
     "start_time": "2023-04-11T08:01:35.051788Z"
    }
   },
   "outputs": [],
   "source": [
    "N1=8\n",
    "N2=8\n",
    "N =N1+N2\n",
    "NS = 1\n",
    "epoch=800001\n",
    "batchsize=4\n",
    "Pos_Vecs_data,Pos_Vecps_data=generate_data_set(Boids1,Boids2,N1,N2,num_of_PosVec=epoch,batch_size=batchsize)\n",
    "Pos_Vecs_tensor=torch.stack(Pos_Vecs_data)\n",
    "Pos_Vecps_tensor=torch.stack(Pos_Vecps_data)\n",
    "#torch.save(Pos_Vecs_tensor, 'Pos_Vecs_tensor.pt')\n",
    "#torch.save(Pos_Vecps_tensor, 'Pos_Vecps_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T09:38:17.163629Z",
     "start_time": "2023-04-11T09:38:13.312469Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(Pos_Vecs_tensor, 'Pos_Vecs_tensor_16birds_2groups_circle_4batch.pt')\n",
    "torch.save(Pos_Vecps_tensor, 'Pos_Vecps_tensor_16birds_2groups_circle_4batch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T15:27:51.553946Z",
     "start_time": "2023-07-16T15:27:50.253537Z"
    }
   },
   "outputs": [],
   "source": [
    "N1=8\n",
    "N2=8\n",
    "N =N1+N2\n",
    "NS = 1\n",
    "epoch=800001\n",
    "batchsize=4\n",
    "Pos_Vecs_tensor=torch.load('./Pos_Vecs_tensor_16birds_2groups_circle_4batch_2.pt',map_location=device)\n",
    "Pos_Vecps_tensor=torch.load('./Pos_Vecps_tensor_16birds_2groups_circle_4batch_2.pt',map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T15:28:01.169330Z",
     "start_time": "2023-07-16T15:27:58.826920Z"
    }
   },
   "outputs": [],
   "source": [
    "count=[np.array([n for n in range(epoch)]) for i in range(batchsize)]\n",
    "for i in range(batchsize):\n",
    "    random.shuffle(count[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "changing observed noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltam=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-16T15:35:02.867Z"
    }
   },
   "outputs": [],
   "source": [
    "N1=8\n",
    "N2=8\n",
    "N =N1+N2\n",
    "NS = 1\n",
    "epoch=400001\n",
    "batchsize=4\n",
    "\n",
    "L = 10\n",
    "hidden_units = 32\n",
    "sigma=1\n",
    "torch.manual_seed(10)\n",
    "\n",
    "experiments=[1,2,3,4]\n",
    "EIs_s=[]\n",
    "\n",
    "for experiment in experiments:\n",
    "    Pos_Vecs_tensor=torch.load('./Pos_Vecs_tensor_16birds_2groups_circle_4batch.pt',map_location=device)\n",
    "    Pos_Vecps_tensor=torch.load('./Pos_Vecps_tensor_16birds_2groups_circle_4batch.pt',map_location=device)\n",
    "    Pos_Vecs_tensor=Pos_Vecs_tensor+Pos_Vecs_tensor*torch.normal(0,deltam,Pos_Vecs_tensor.shape,device=device)\n",
    "    Pos_Vecps_tensor=Pos_Vecps_tensor+Pos_Vecps_tensor*torch.normal(0,deltam,Pos_Vecs_tensor.shape,device=device)\n",
    "\n",
    "    count=[np.array([n for n in range(epoch)]) for i in range(batchsize)]\n",
    "    for i in range(batchsize):\n",
    "        random.shuffle(count[i])\n",
    "\n",
    "    MAE = torch.nn.L1Loss(reduction='none')\n",
    "    MAE_raw = torch.nn.L1Loss()\n",
    "\n",
    "    net= Parellel_Renorm_Dynamic(sym_size = 4*N, latent_size = 4*2, effect_size = 4*N, cut_size = 2,\n",
    "                         hidden_units = 32, normalized_state = False, device = device)\n",
    "    net= net.to(device) if use_cuda else net\n",
    "\n",
    "    net.load_state_dict(torch.load('./Models_16birds_2groups/model_16birds_2groups_straight_tensor_kde_4batch_4layer_16p_short_lr54_962000.pkl',map_location=device))\n",
    "\n",
    "    Pos_Vecs=[]\n",
    "    Pos_Vecs_predicts_l=[]\n",
    "    EIs=[]\n",
    "    CEs=[]\n",
    "    loss_s=[]\n",
    "\n",
    "    weights = [torch.ones(4,1, device=device)/4]\n",
    "    weights2 = [torch.ones(4,1, device=device)/4]\n",
    "\n",
    "    optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)\n",
    "    for t in range(epoch):\n",
    "        Pos_Vec=torch.stack([Pos_Vecs_tensor[count[i][t],i] for i in range(batchsize)])\n",
    "\n",
    "        Pos_Vec_p=torch.stack([Pos_Vecps_tensor[count[i][t],i] for i in range(batchsize)])\n",
    "\n",
    "        Pos_Vec=Pos_Vec.contiguous().view(batchsize, 4*(N1+N2))\n",
    "\n",
    "        Pos_Vec_p=Pos_Vec_p.contiguous().view(batchsize, 4*(N1+N2))\n",
    "\n",
    "        predicts,latents,latent_ps=net(Pos_Vec)\n",
    "        #predicts_0,latents_0,latent_ps0=net.back_forward(Pos_Vec_p)\n",
    "\n",
    "        losses_returned,loss1 = net.loss_weights(predicts, Pos_Vec_p, weights,MAE,3)\n",
    "        #losses_returned_0,loss2 = net.loss_weights_back(predicts_0, Pos_Vec,weights, MAE,3)\n",
    "        loss=loss1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        optimizer.step()\n",
    "        if t % 10000 == 0:\n",
    "\n",
    "            eis_kde,sigmass_kde,weights=net.calc_EIs_ked(Pos_Vec,Pos_Vec_p,latent_ps,1,MAE,L,10*L,3,device)\n",
    "            print('iter %s:' % t, 'loss = %.3f' % loss)\n",
    "            print('dEIs kde = ', [ei_kde[0] for ei_kde in eis_kde])\n",
    "            print('sigmas kde=', [sigma_kde.mean() for sigma_kde in sigmass_kde])\n",
    "            #print(weights)\n",
    "            #print('Causal Emergence = ', [ei[0] - eis[0][0] for ei in eis])\n",
    "            with open('./Data/16birds_2groups_circle_reweight_REMAKE_ovserved.txt', 'a') as file1:\n",
    "                file1.write(\n",
    "                    str(t)+ ' ' +str(loss.item()) + ' ' + str([ei[0] for ei in eis_kde]) + '\\n')\n",
    "            #if t % 10000 == 0:\n",
    "                #torch.save(net.state_dict(), './Models_16birds_2groups/model_16birds_2groups_circle_2path_2angle_tensor_kde_4batch_4layer_reweight_REMAKE_1layer_%s.pkl'%(t))\n",
    "\n",
    "            EIs.append([ei[0] for ei in eis_kde])\n",
    "            #CEs.append([ei[0] - eis[0][0] for ei in eis])\n",
    "            loss_s.append(loss1)\n",
    "    EIs_s.append(EIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-10T02:40:50.773768Z",
     "start_time": "2023-06-10T02:40:50.767460Z"
    }
   },
   "outputs": [],
   "source": [
    "EIs_s_array=np.array(EIs_s)\n",
    "np.save('EIs_s_array_16birds_2groups_circle_observed_001.npy',EIs_s_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9d9229479f13ffc81c98d99396af8142d82ff4fad005cc72f300e12d6f225f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
